#!/usr/bin/env python3
"""\
authors:	Mouhamadou Thiam
date:		06.10.2024
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.by import By
# from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from pathlib import Path
import pandas as pd
import time

# Set up Chrome options for Selenium
chrome_options = Options()
#chrome_options.add_argument("--headless")  # Run in headless mode (no GUI)
chrome_options.add_argument("--no-sandbox")

# chrome_options = webdriver.ChromeOptions()
#chrome_options.add_experimental_option("prefs", {"profile.default_content_setting_values.cookies": 2})


# Initialize the Selenium WebDriver
# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
driver = webdriver.Chrome()

# Load the Zurich Airport arrivals page
url = "https://www.flughafen-zuerich.ch/en/passengers/fly/flightinformation/arrivals"
driver.get(url)

# Wait for the page to fully load with explicit waits
wait = WebDriverWait(driver, 20)

# Wait for the planned header to load
wait.until(lambda x: x.find_element(By.CSS_SELECTOR, ".flightListTable__cellPlanned"))
time.sleep(5)  # Adjust sleep if needed to give the page enough time to load

# If cookie button appears, click accept all
try:
    print("Cookie must be accepted")
    #driver.find_element(By.CSS_SELECTOR, ".CybotCookiebotDialogBodyButton").click()
except:
    print("No cookie window")

# Use Selenium to get the page source and parse it with BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Close the Selenium browser once the page is fully loaded
#driver.quit()

# Find the table rows containing the flight information
rows = soup.find('tbody', class_='css-1p9smcc')

# Initialize a list to store the scraped data
flights_data = []

# Loop through each row and extract the necessary information
for row in rows.contents:
    try:
        txt = row.find('td', class_='flightListTable__cellPlanned').text
        if row.find('td', class_='flightListTable__cellPlanned').find('div', class_='crossed') is not None:
            txt = row.find('td', class_='flightListTable__cellPlanned').find('div', class_='crossed').text.strip()
        planned = txt
        expected = row.find('td', class_='flightListTable__cellEstimated').text.strip()
        arriving_from = row.find('td', class_='flightListTable__cellAirport').text.strip()
        src = ""
        if row.find('td', class_='flightListTable__cellAirline').next.has_attr('src'):
            src = row.find('td', class_='flightListTable__cellAirline').next['src']
        else:
           continue
        airline = Path(src).stem
        # flight_number = row.find('div', class_='flightListTable__cellFlightNumber').text.strip()
        status = row.find('td', class_='flightListTable__cellStatus').next.text.strip()

        # Append the scraped data to the list
        flights_data.append({
            'Planned': planned,
            'Expected': expected,
            'Arriving from': arriving_from,
            'Airline': airline,
            #'Flight number': flight_number,
            'Status': status
        })
    except AttributeError:
        print(row.prettify())
        # In case some fields are missing, handle them gracefully
        continue

# Convert the scraped data into a pandas DataFrame
df = pd.DataFrame(flights_data)

# Save the data to a CSV file (optional)
df.to_csv('zurich_airport_arrivals.csv', index=False)

# Display the DataFrame
print(df)
